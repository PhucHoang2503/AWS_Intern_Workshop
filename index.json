[{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/5-workshop/5.1-workshop-overview/","title":"1. Workshop Overview","tags":[],"description":"","content":"Hands-on workshop Overview In this workshop, we will practice deploying a static website on Amazon S3 storage service and optimizing content delivery via Amazon CloudFront Content Delivery Network (CDN).\nKey Contents: Preparation (5.2): Create an S3 Bucket and upload sample website source code to the bucket. Configure S3 Static Hosting (5.3): Enable static website hosting feature on S3. Access Management (5.4 - 5.5): Configure Block Public Access and Access Control Lists (ACL) to allow users to access the website from the internet. Verify Website (5.6): Verify the website operation via the S3 Endpoint. CloudFront Integration (5.7): Initialize a CloudFront Distribution pointing to the S3 origin. Enhance security by blocking direct public access to S3 (Block Public Access) and routing traffic through CloudFront. Cleanup (5.8): Instructions on deleting created resources to avoid incurring costs. "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/5-workshop/5.7-cloudfront/5.7.1-block-public-access/","title":"Block Public Access","tags":[],"description":"","content":" In S3 Bucket Interface Select Permissions Currently, the Block all public access function is Off because we disabled it in step 4 Select Edit Select Block all public access Select Save changes - Another window appears to confirm the edit, type confirm - Select Confirm\n- Block all public access is now enabled, at this point no public access can connect to your S3 Bucket.\nCheck Block all public access function In S3 bucket interface Select Properties - Scroll to the bottom of the page, at the Static website hosting section - Select the square icon to copy URL\n- Open URL in a new browser tab\n-\u0026gt; Congratulations, you have successfully Blocked public access\n"},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Enhance governance with metadata enforcement rules in Amazon SageMaker by Pradeep Misra, Ramesh H Singh, and Sandhya Edupuganti | on 28 MAR 2025 in | Amazon SageMaker Lakehouse, Analytics, Announcements, Technical How-to\nThe next generation of SageMaker brings together widely adopted AWS machine learning and analytics capabilities, delivering an integrated experience with unified access to all data. Amazon SageMaker Lakehouse supports unified data access, and Amazon SageMaker Catalog, built on Amazon DataZone, offers catalog and governance features to meet enterprise security needs. Amazon SageMaker Catalog now supports metadata rules allowing organizations to enforce metadata standards across data publishing and subscription workflows.\nA rule is a formal agreement that enforces specific metadata requirements across user workflows (e.g., publishing assets to the catalog, requesting data access) within the Amazon SageMaker Unified Studio portal. For instance, a metadata enforcement rule can specify the required information for creating a subscription request or publishing a data asset or a data product to the catalog, ensuring alignment with organizational standards. Metadata rules also enable the creation of custom approval workflows for subscriptions to assets, using collected metadata to facilitate access decisions or auto-fulfillment—outside of SageMaker.\nBy standardizing metadata practices, Amazon SageMaker Catalog enables customers to meet compliance requirements, enhance audit readiness, and streamline access workflows for greater efficiency and control. One such customer is Amazon Shipping Tech, which uses SageMaker Catalog for cataloging, discovery, sharing, and governance across their data ecosystem:\n“We’re building an Analytics Ecosystem to drive discovery across the organization—but without consistent metadata, even our most valuable data can go unused. This feature empowers more teams to actively contribute to metadata curation with the right governance in place. It allows us to set clear standards for data producers while streamlining the collection of required subscription details—no extra templates needed.\nBy enforcing standard metadata attributes, we improve discoverability, add context to each request, and strengthen support for analytics and GenAI solutions.”\n— Saurabh Pandey, Principal Data Engineer at Amazon Shipping Tech\nSample use-cases Metadata rules could help in the following use cases:\nA producer at an automobile company is preparing to publish a new dataset into the organization’s data catalog. The domain owner for the automotive domain requires that the producer include metadata fields such as Model Year, Region, and Compliance Status. Before the dataset can be published, automated checks make sure that these fields are correctly filled out according to the predefined standards. A consumer is requesting access to data assets in SageMaker. To meet organization standards and support audit and reporting needs, they must complete the subscription request, fill out a detailed form that includes the project purpose, and attach an email link with pre-approval and compliance training evidence to request subscription for the financial data product. The data owner reviews the request, checking that all required metadata are provided before granting access. Key benefits Key benefits of new metadata enforcement rules include:\nEnhanced control for domain (unit) owners – Admins can enforce additional metadata fields on subscription and publishing workflows, which must be adhered to by data users. This process supports thorough reviews and enforces organizational compliance. Custom workflow support – You can create custom workflows for fulfilling subscriptions on non-managed assets by capturing essential metadata from data consumers. This metadata is used to configure access or support specific business requirements. In this post, we guide you through two workflows: setting up metadata enforcement rules for a specific domain and publishing an asset or data product in a catalog, and setting up metadata enforcement rules for a specific domain and subscribing to an asset or data product that is owned by a project within that domain.\nSolution Overview: Metadata Enforcement for Publishing In this solution, we’ll walk through two workflows: setting up metadata enforcement for publishing, and setting up metadata enforcement for subscription.\nPrerequisites To follow this post, you should have a SageMaker Unified Studio domain set up with a domain owner or domain unit owner privileges. For instructions, refer to the Getting started guide.\nSet up metadata enforcement for publishing In this section, we show you how to set up metadata rules for a specific domain as a domain admin. We also explain what happens when you publish an asset or data product in a catalog with these rules applied.\nCreate a domain unit for the marketing team As a domain admin, complete the following steps:\nOn the SageMaker Unified Studio console, choose the Govern dropdown menu and choose Domain units. Choose CREATE DOMAIN UNIT. Provide details shown in the following screenshot and choose CREATE DOMAIN UNIT. You can see the domain unit as shown in the following screenshot. Enable a metadata form creation policy in the Marketing domain unit Navigate to the AUTHORIZATION POLICIES tab in the Marketing domain unit and choose Metadata form creation policy. Choose ADD POLICY GRANT. Select All projects in a domain unit and add a policy grant. You can also select specific projects that can create metadata forms. Choose ADD POLICY GRANT. You can see the policy now created for the Marketing domain unit. Create a metadata form to be enforced for assets before publishing In the publish-1 project, choose Metadata entities under Project catalog in the navigation pane. On the Metadata forms tab, choose CREATE METADATA FORM. Provide a display name, technical name, and description. Choose CREATE METADATA FORM. After you create the form, you can choose CREATE FIELD to enforce fields that should be there in all published assets. Provide details as shown in the following screenshot. Select Searchable, Required, and Publishing because these fields are required before publishing. Choose CREATE FIELD. Add another field as shown in the following screenshot. Both fields created with the Publishing action will require values before publishing to the catalog. Create rules for asset publishing In the publish-1 project, under Domain Management in the navigation pane, choose Domain units. Choose the Marketing domain unit. On the Rules tab, choose ADD. Create the rule configuration with details in the following screenshot and add the metadata form created in the previous step. You can select the scope of enforcement by asset type and projects. Choose ADD RULE to create the rule. The publishing enforcement rule publish_rules is now created. Create a project in the Marketing domain unit Create a project named publish-1 in the Marketing domain unit. To learn how to create a project, refer to Create a project.\nCreate an asset in the project Rules work on assets managed by the SageMaker Catalog or on custom assets. To create an asset, complete the following steps:\nIn the publish-1 project, choose Assets under Project catalog in the navigation pane. On the Create dropdown menu, choose Create asset. Provide an asset name and description, then choose Next. For this solution, you will create an Amazon Simple Storage Service (Amazon S3) object collection. For Asset type, choose S3 object collection. For S3 location ARN, enter the Amazon Resource Name (ARN) of the S3 object. Choose Next. Choose CREATE. The asset marketing_campaign_asset is now created. This is still an inventory asset and not published to the catalog. Publish rules enforcement Asset details now show that the required values are missing for the mandatory form Publish_form. You can try to publish without the required fields and the system will throw an error to enforce publishing metadata rules. To fix the issue, edit the value for the metadata form to provide the required info. Provide details for the fields and choose SAVE. Choose PUBLISH ASSET now and the asset will be published to the catalog. You can see the asset is published with the required fields enforced with rules. Set up metadata enforcement for subscription requests In this section, we show you how to set up metadata rules for a specific domain as a domain admin. We also explain what happens when you subscribe to an asset or data product with these rules applied.\nCreate rules for asset subscription Navigate to the project used in the previous section and choose Metadata entities under Project catalog in the navigation pane. On the Metadata forms tab, choose CREATE METADATA FORM to create a new form. Provide a form name and description, then choose CREATE METADATA FORM. Add fields to the form by choosing CREATE FIELD and turning on Enabled. Add a field for subscribers to explain the use case when requesting access. Create rules for asset subscription Complete the following steps:\nOn the project page, choose Domain units under Domain Management in the navigation pane. Choose the Marketing domain unit. We already have a publishing rule. On the Rules tab, choose ADD to add a new rule. Provide details for the new rule. Specify the action as Subscription request. Add the metadata form created in the previous steps (Subscribe_form). Choose the scope and projects for enforcement as shown in the following screenshot. Choose ADD RULE. You will see the subscription enforcement rule is now created. Subscribe the asset Complete the following steps to subscribe the asset:\nOn the project page, navigate to the marketing asset. Choose SUBSCRIBE. The subscribe form is now attached in the request for the user to provide information. After a data consumer submits a subscription request, the data producer receives it along with the provided metadata—such as Use Case. This allows producers to review the request before granting access.\nClean up To avoid incurring additional charges, delete the Amazon SageMaker domain. Refer to Delete domains for the process.\nConclusion In this post, we discussed metadata rules and how to implement them for both publishing and subscribing to assets across different domains, demonstrating effective metadata governance practices.\nThe new metadata enforcement rule in Amazon SageMaker strengthens data governance by enabling domain unit owners to establish clear metadata requirements for data users, streamlining catalog health and enhancing data governance process for access request. This feature enables organizations to align with organization’s metadata standards, implement custom workflows, and provide a consistent, governed data workflow experience.\nThe feature is supported in AWS Commercial Regions where Amazon SageMaker is currently available. To get started with metadata rules—\nRead the user guide for creating rules in the publishing workflow\nRead the user guide for creating rules in subscription requests\nAbout the Authors Pradeep Misra is a Principal Analytics Solutions Architect at AWS. He works across Amazon to architect and design modern distributed analytics and AI/ML platform solutions. He is passionate about solving customer challenges using data, analytics, and AI/ML. Outside of work, Pradeep likes exploring new places, trying new cuisines, and playing board games with his family. He also likes doing science experiments, building LEGOs and watching anime with his daughters.\nRamesh H Singh is a Senior Product Manager Technical (External Services) at AWS in Seattle, Washington, currently with the Amazon SageMaker team. He is passionate about building high-performance ML/AI and analytics products that enable enterprise customers to achieve their critical goals using cutting-edge technology. Connect with him on LinkedIn.\nSandhya Edupuganti is a Senior Engineering Leader spearheading Amazon DataZone (aka) SageMaker Catalog. She is based in Seattle Metro area and has been with Amazon for over 17 years leading strategic initiatives in Amazon Advertising, Amazon-Retail, Latam-Expansion and AWS Analytics.\n"},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/5-workshop/5.2-prerequiste/5.2.1-create-s3-bucket/","title":"Create S3 Bucket","tags":[],"description":"","content":"Create S3 Bucket What is an S3 Bucket? An S3 bucket is a container for objects stored in Amazon S3. Think of it as a top-level folder containing your files. Each bucket has a globally unique name and exists in a specific AWS Region. Buckets serve as the fundamental organizing unit in S3 and provide a namespace for object storage.\nStep-by-Step Instructions Access S3 Service\nGo to AWS Management Console Find S3 using the search bar or by navigating to the Storage service category Select S3 Initiate Bucket Creation\nIn the S3 interface, select Create bucket Configure Basic Bucket Settings\nIn the Create bucket interface: Bucket name: Enter workshop-demo-092025 (if the name is taken, add a number or use a custom name) AWS Region: Select the region closest to your users or as required Object Ownership: Select ACLs disabled (recommended) Verify Bucket Creation Success! You have created an S3 bucket to store website source code. The bucket will be displayed under General purpose buckets\nWhat has been achieved Created a globally unique S3 bucket Configured default S3 settings Set up the foundation for static website hosting Next Step Next, we will upload the website source code to the S3 bucket and store it.\n"},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Tran Hoang Phuc\nPhone Number: 0792473180\nEmail: bovncraft123@gmail.com\nUniversity: FPT University HCM\nMajor: Information Technology\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Enhance resiliency and data protection for SAP HANA high-availability deployments with AWS Backup for SAP HANA by Vijay Sitaram, Amit Mohanty, Archana Kuppuswamy, and Khaliluddin Siddiqui | on 01 APR 2025 | in AWS Backup, AWS Support, Best Practices, SAP on AWS, Technical How-to, Thought Leadership\nIntroduction SAP HANA databases are often deployed in High Availability (HA) configurations to maintain continuous operations on AWS. SAP HANA databases need both HA configuration and comprehensive backup strategies to protect data and support complete system recovery. Despite the benefits of SAP HA deployments, SAP HANA databases remain vulnerable to various risks, including cluster-wide failures, logical errors, data corruption, ransomware attacks, and compliance issues.\nHA alone cannot address scenarios such as point-in-time recovery needs, system refreshes for test/development environments, or long-term data retention requirements. Organizations need a holistic strategy for complete data protection, regulatory compliance, and operational resilience.\nFor RISE with SAP on AWS deployments, database backups are managed through AWS Backint Agent for SAP HANA. This is the standard backup solution that integrates with SAP HANA databases. The backup process is fully managed by SAP as part of the RISE offering, but customers can monitor backup status through SAP-provided tools and AWS Backup console. For customers running SAP natively on AWS such as SAP ERP or S/4HANA, AWS Backup for SAP HANA is a great option.\nIn this blog, you will learn about how organizations are implementing robust backup strategies for their SAP HANA databases with AWS Backup in HA deployment. The blog also covers the data protection and Well-Architected Framework for SAP lens Reliability pillar and Operational best practices.\nData Protection and Well-Architected Framework for SAP Lens The Well-Architected Framework SAP Lens emphasizes database backups primarily within its Reliability pillar. This pillar is provided expert guidance for SAP workloads to perform their intended functions consistently and correctly, even when faced with failures or disruptions.\nWithin the Reliability pillar, database backups are highlighted as a key component of several critical areas:\nData Protection: The framework provides best practices for implementing robust backup and recovery strategies for SAP databases and other critical data. This provides guidance for organizations to prepare for data recovery in case of system failures, data corruption, or other unforeseen events. Disaster Recovery: Backups play a vital role in disaster recovery planning. The SAP Lens offers guidance on setting up disaster recovery mechanisms that leverage backups to restore operations in case of major incidents. High Availability: While not directly about backups, high availability strategies complement backup procedures by ensuring system uptime and minimizing the need for recovery from backups. Data Retention: The framework provides recommendations for long-term data storage and archiving, which often involve backup strategies to meet regulatory requirements and business needs. The Reliability pillar emphasizes several key aspects of database backup strategies:\nImplementing regular and consistent backup schedules Thoroughly testing recovery procedures for effectiveness Automating backup solutions to reduce human error and maintain consistency Maintaining data integrity and security throughout the backup and recovery processes Aligning backup strategies with the organization’s Recovery Time Objectives (RTO) and Recovery Point Objectives (RPO) By focusing on these areas, the SAP Lens of the Well-Architected Framework provides comprehensive guidance such that SAP workloads on AWS are resilient and recoverable. It helps organizations implement effective backup strategies that support business continuity and minimize data loss risks. This approach delivers rapid restoration of critical SAP systems during failures, preserving the reliability and availability essential for businesses’ core operations.\nPrimer on AWS Backint Agent for SAP HANA AWS Backint for SAP HANA offers native integration with SAP HANA’s backup capabilities, ensuring application-consistent backups. The solution utilizes Amazon Simple Storage Service (S3) for storage, offering durability, scalability, and cost-effectiveness. The Backint agent supports incremental backups, reducing storage costs and backup windows. It also offers encryption for backups both in transit and at rest, enhancing security.The solution is scriptable, allowing automation and integration with existing backup workflows. It supports Point-In-Time Recovery (PITR), crucial for addressing data corruption or human errors. Backint scales with large SAP HANA databases, minimizing impact on performance during backup operations. Cost-effectiveness is a key feature, leveraging S3 storage class and life cycle policy to transition to cheaper storage classes for long-term retention.The solution benefits from the high durability and availability of Amazon S3 for backup storage.\nThese features make AWS Backint Agent a great choice for customers seeking a reliable, SAP-native backup solution that leverages AWS storage capabilities, particularly those preferring direct integration with SAP tools and having simpler backup requirements. To learn more about how to configure AWS Backint agent for SAP with Amazon S3, please visit Backup and restore SAP HANA workloads to Amazon S3.\nAWS Backup for SAP HANA on Amazon Elastic Cloud Compute (EC2) AWS Backup for SAP HANA on Amazon EC2 delivers a managed, streamlined experience for SAP HANA database backup and restore operations. It enhances the customer experience through its intuitive AWS Backup console, extends data protection and simplifies management across multiple AWS services, including Amazon EC2, Amazon Elastic Block Store (EBS), and Amazon Elastic File System (EFS). The continuous backup features offer substantial cost savings, particularly for users previously limited to full backups with AWS Backint Agent. AWS Backup supports comprehensive audit and compliance capabilities for backups with Backup Vault Lock and Legal Holds. It supports automated cross-region cross-account backup copying, enhancing disaster recovery and data redundancy. This full-managed service optimizes SAP HANA data management, improving reliability and operational efficiency within the AWS ecosystem.\nTo learn more about how to automate and simplify SAP HANA backups with AWS Backup, please read the blog Automate and Simplify SAP HANA Backups with AWS Backup. For documentation on how to configure AWS Backup for SAP HANA, please read the document.\nRecommendations for SAP HA deployment AWS Backup for SAP HANA supports both single-node and HA deployments. In HA deployment, it automatically identifies the active node for running backups as shown in Figure-3 below. Customers use Pacemaker cluster software to manage the SAP HANA HA deployment. Figure-1: AWS Backup for SAP HANA HA deployment\nBelow are the steps to configure AWS Backup for SAP HANA on Amazon EC2:\nSave HANA Credentials in Secrets Manager Check IAM permissions and tags for EC2 instances, and Secrets key Register one of the HANA instances in HA deployment with SSM for SAP (ssm-sap) Install Backint for AWS Backup using SSM document (AWSSAP-InstallBackintForAWSBackup) OPT in SAP HANA on Amazon EC2 in AWS Backup console settings Create a backup plan. To initiate an on-demand backup of SAP HANA database in HA deployment from AWS Backup console. Scheduled backups are configured in Backup plan by defining a backup policy. Following is a high-level list of activities:\nAWS Backup calls SSM for SAP backup APIs to initiate backup. AWS SSM for SAP calls Backint via a SSM run command. Backup is stored in AWS Vault and backup status is recorded. Below steps are required before and after database restore in SAP HA deployment when managed by Pacemaker cluster manager. To learn more about additional steps to include when restoring a HA system of SAP HANA, please read the documentation SAP HANA HA restore. In addition, you will find more guidance on setting up Pacemaker cluster in SAP HANA on AWS: High Availability Configuration Guide for SLES and RHEL.\nBefore restore: SAP database cluster and the nodes (primary database node and standby database node) must be placed in maintenance mode, and SAP HANA is stopped on both nodes. After restoring system database, start SAP HANA on primary database before proceeding to restore tenant database. During the database restore operation, the SAP database cluster and the cluster nodes remain in maintenance mode.\nTo initiate SAP HANA database restore, login to AWS Backup console and initiate a restore by using a full backup or a continuous backup recovery point ID. Following is a high-level list of activities:\nAWS Backup calls SSM for SAP backup APIs to initiate restore. AWS SSM for SAP calls Backint via a SSM run command. Database is restored from AWS Vault to the primary database node. After restore: Following a successful restore of system and tenant database, stop SAP HANA in secondary database node. Register SAP HANA System Replication (HSR) in primary database node followed by secondary database node. Proceed to start SAP HANA in secondary database node, and take SAP database cluster nodes and global cluster out of maintenance mode.\nSimplify data protection management and operations AWS Backup for SAP HANA is a fully managed AWS service that provides robust set of features to streamlines backup processes, enhance recovery, and improve disaster recovery capabilities for SAP HANA single-node and HA deployments.\nFeature Benefits Centralized Management Single dashboard for managing backup and restores across multiple SAP instances simplifies recovery operations in distributed architectures. Cost optimized storage Lifecycle policy management features allow for cost-effective long-term retention of backups without compromising recovery capabilities. Cross-Region and Cross-Account copy Native integration with SAP HANA creates application-consistent backups, critical for maintaining data integrity in complex SAP environments. Cross-region and cross-account backup capabilities enable geographically dispersed recovery options, enhancing disaster recovery strategies and ransomware protection. Compliance and Audit Helps meet regulatory requirements with audit logs and retention policies, crucial for many SAP deployments. Vault Lock is a mechanism that allows you to enforce retention policies and prevent deletion or modification of backups stored in an AWS Backup vault. Supports placing legal holds on backups to prevent deletion regardless of retention settings. Monitoring and alerting Default AWS Backup console Job dashboard to monitor backup and restore job status, and recovery point id status. CloudWatch metrics and alarms to monitor failed backup, restore and recovery points status. Point-In-Time Recovery (PITR) Automated and scheduled PITR continuous backups to maintain recovery point IDs. PITR allows for quick SAP HANA database restore to a specific timestamp within one second granularity, minimizing downtime. Policy based automation Reduces human error in backup and recovery processes through automation and predefined policies. Resource tagging Supports up to 50 tags for SAP HANA backups, and AWS service such as Amazon EC2 Amazon Machine Image (AMI) backups, Amazon EBS volume snapshots and Amazon EFS filesystem snapshots. Simplified Recovery management Recovery Time Objective (RTO): The combination of continuous backup (full and incremental) and database logs with an automated processes significantly reduces the time to recover SAP systems. Recovery Point Objective (RPO): Automated scheduled continuous backups allow for more recent recovery points and logs, minimizing potential data loss up to 1 second granularity. Scalability Easily scales to handle large SAP HANA databases without compromising recovery speed or reliability. Scheduling Scheduled from AWS Backup console for hourly, daily, weekly and custom cron expressions. Security Built-in encryption with KMS and IAM integration provides secure storage and recovery processes, maintaining data integrity and access control. Support for additional SAP components Seamless integration with other AWS services enables advanced recovery scenarios and automation such as Amazon EC2, Amazon EBS, and Amazon EFS. AWS Backup for SAP HANA and AWS Backint Agent offer distinct approaches to backing up SAP HANA databases. Backint is SAP HANA-specific, integrating seamlessly with the SAP ecosystem and using SAP tools for management and recovery. In contrast, AWS Backup for SAP HANA is a more versatile service, providing backup capabilities across multiple AWS services, including native SAP HANA. It offers greater flexibility in storage and lifecycle options, managed through the AWS console.\nBelow is a side-by-side comparison of AWS Backint Agent and AWS Backup for SAP HANA:\nFeature AWS Backint Agent for SAP HANA AWS Backup for SAP HANA Automation SAP HANA’s scheduling mechanisms Backup plan configured with Backup rule Backup Types Full, incremental, differential and logs Full, incremental and logs Catalog management Stored in SAP HANA backup catalog Stored in AWS Backup catalog Compliance Both support audit logs and retention policies such as legal hold Disaster Recovery Amazon S3 Cross-Region Replication (CRR) AWS Backup copying backups across regions and across accounts to same or different AWS Vault with same or different KMS key. End-user tool Backup and recovery managed for SAP HANA database from SAP HANA Studio. Backup and recovery management from AWS Backup console for SAP HANA and additional SAP components such as Amazon EC2, Amazon EBS and Amazon EFS. Encryption Amazon S3 bucket server-side encryption SAP HANA database backups are always encrypted. AWS Key Management Service (KMS) encryption is configured in AWS Backup Vault Granularity File-level and database-level backups Database-level backups, and backups for SAP components such as Amazon EC2, Amazon EBS, and Amazon EFS Lifecycle management Amazon S3 lifecycle policy AWS Backup Vault lifecycle policy Performance Optimized for SAP HANA-specific backup operations Optimized for SAP HANA backup operations, and AWS services (Amazon EC2, Amazon EBS, Amazon EFS) Recovery Full database and PITR recovery from SAP HANA Studio Full database and PITR recovery from AWS Backup console Scalability Designed specifically for SAP HANA database Designed for SAP HANA database and AWS services such as Amazon EC2, Amazon EBS and Amazon EFS Storage Backups stored directly in Amazon S3 bucket managed by customer Backup stored directly in AWS Backup Vault managed by service Security Amazon S3 Object Lock using Write-Once-Read-Many (WORM) model. Supports IAM file-grained access control AWS Backup Vault Lock for additional security and control over backup images. Supports IAM file-grained access control Operational best practices While AWS Backint Agent is deeply integrated with SAP, AWS Backup extends this capability and integrates more broadly with AWS services. AWS Backup offering wider cross-service functionality and Backint providing specialized SAP-centric features makes it suitable option for SAP HANA backup strategy. Below are operational best practices for AWS Backup for SAP HANA on Amazon EC2.\nDiscovery and registration: AWS Backup for SAP HANA necessitates public endpoint access for several AWS services, including SSM for SAP (SSM4SAP), Systems Manager (SSM), Secrets Manager, and AWS Backup. To enable smooth operation, configure firewall to allow access for both SAP HANA nodes to permit service and storage endpoint access. The AWS SSM Agent must be installed and running on EC2 instances hosting SAP HANA database, with the correct IAM roles attached. It’s required to meet these prerequisites on all HANA nodes to facilitate registration of SAP HANA database with AWS SSM4SAP and run successful backup and restore operations through AWS Backup.\nCost-optimized storage: Optimize costs and streamline administration with AWS Backup Vault’s lifecycle and storage tiers for SAP HANA environments. Implement a comprehensive tiered Backup Plan that incorporates both full database backups and PITR backups aligned with your organization’s retention policy. This strategy enables robust data protection while effectively managing expenses. For full backups, leverage backup lifecycle policies to automatically transition older backups to cost-effective cold storage solutions like Amazon S3 Glacier. This approach significantly reduces long-term storage costs without compromising data accessibility. While storage tiering isn’t supported for PITR (continuous backups), PITR will determine type of database backup (full or incremental) intelligently based on volume of changes written to database, and combined with lifecycle policy optimizes cost of storage.\nSecurity, Governance and Compliance: AWS Backup Vault Lock establishes strong governance frameworks, aligning data protection with stringent compliance requirements. The Vault Lock’s compliance and governance modes enable organizations to meet industry-specific regulations effectively. Legal hold functionality prevents deletion of backups under review, ensuring data integrity during audits or legal proceedings. Implement AWS Backup Vault secure encrypted vaults and immutable backups. This multi-layered approach safeguards SAP data against unauthorized access, ransomware attacks, and both internal and external threats. By default, SAP HANA backups are encrypted, and AWS Backup Vault mandates the use of AWS Key Management Service (KMS) for encrypting backup storage, ensuring data confidentiality at rest.\nDisaster Recovery (DR) with cross-account and cross-region copy: This multi-region backup strategy significantly enhances business continuity and resilience against regional outages. The cross-region cross-account functionality allows backups to be automatically copied to different AWS regions and account, ensuring data availability even if an entire region becomes unavailable. Cross-account copying adds another layer of protection by storing backups in separate AWS accounts, isolating them from potential security breaches in the primary account. To learn how to set-up cross-region and cross-account copy, please refer to this blog.\nPerformance tuning: It is recommended to right-size HANA database EC2 instance and EBS volumes to achieve optimal performance for day-to-day operations and backups. For SAP-certified memory optimized EC2 instances, take into account EBS specifications for maximum bandwidth, maximum throughput and maximum IOPS for instance type. This is helpful in understanding the limits for EC2 instance when tuning individual EBS volumes for SAP HANA data, and logs. This has direct impact on performance of any workload, including backups. To measure EBS metrics for individual volumes and aggregated metrics at instance level, please refer to AWSSupport-CalculateEBSPerformanceMetrics runbook which helps diagnose Amazon Elastic Block Store (Amazon EBS) performance issues by calculating and publishing performance metrics to an Amazon CloudWatch dashboard.\nFigure-2: AWSSupport-CalculateEBSPerformanceMetrics runbook Amazon CloudWatch Dashboard\nCarefully review and adjust these parameters associated with SAP HANA and AWS Backint Agent so that they meet performance and recovery time objectives. The configuration parameters to fine-tune SAP HANA are stored in /hana/shared/$SID/global/hdb/custom/config/global.ini, and for AWS Backint Agent in /hana/shared/aws-backint-agent/aws-backint-agent-config.yaml. These parameters should be adjusted based on system resources (CPU, memory, network capacity), database size and workload characteristics, Backup window requirements, and available network bandwidth. To learn more about tuning performance of SAP HANA backup and restore, please refer to Install and configure AWS Backint Agent for SAP HANA – Performance tuning and Install and configure AWS Backint Agent for SAP HANA – Backint-related SAP HANA parameters.\nParameter configuration for SAP HANA Backup and Restore optimization\nConfiguration File Parameter Purpose Setting global.ini parallel_data_backup_backint_channels Controls number of parallel backup channels 8 (adjust based on system resources) global.ini data_backup_buffer_size Sets buffer size for backup operations 1024 (adjust based on available memory) aws-backint-agent-config.yaml UploadChannelSize Controls size of upload chunks 256MB (Adjust as needed) aws-backint-agent-config.yaml UploadConcurrency Number of concurrent upload streams 8 (Adjust as needed) aws-backint-agent-config.yaml MaximumConcurrentFilesForRestore Controls parallel restore operations 5 (Adjust based on system resources) aws-backint-agent-config.yaml DownloadConcurrency Number of concurrent download streams 10 (Adjust based on system resources) Monitoring: AWS Backup Dashboards centralize backup activity across AWS services. They track job statuses, monitor compliance, and showcase protected resources. The dashboards identify issues, monitor policy adherence, and manage operations. AWS Backup integrates with Amazon CloudWatch with metrics and events, allowing you to track backup and restore job statuses, and set alarms for failures.\nFigure-3: Amazon CloudWatch dashboard to monitor AWS Backup for SAP HANA Backup, Restore and Copy jobs\nTroubleshooting errors: When configuring AWS Backup for SAP HANA on Amazon EC2, take note of the following log files and the purpose of these files.\nActivity Related logs Discovery and Registration /usr/bin/ssm-sap/logs/discovery.log Install AWS Backint Agent for SAP HANA /var/lib/amazon/ssm/packages/AWSSAP-Backint/*/aws-backint-agent-install-*.log Backup and Restore log for AWS Backint Agent /hana/shared/aws-backint-agent/aws-backint-agent.log SAP HANA logs for system and tenant database System DB backup and recovery logs: /usr/sap/\u0026lt;SID\u0026gt;/\u0026lt;SID\u0026gt;\u0026lt;instance\u0026gt;/\u0026lt;hostname\u0026gt;/trace/backup.log /usr/sap/\u0026lt;SID\u0026gt;/\u0026lt;SID\u0026gt;\u0026lt;instance\u0026gt;/\u0026lt;hostname\u0026gt;/trace/backint.log Tenant DB backup and recovery logs: /usr/sap/\u0026lt;SID\u0026gt;/\u0026lt;SID\u0026gt;\u0026lt;instance\u0026gt;/\u0026lt;hostname\u0026gt;/trace/DB*\u0026lt;SID\u0026gt;/backup.log /usr/sap/\u0026lt;SID\u0026gt;/\u0026lt;SID\u0026gt;\u0026lt;instance\u0026gt;/\u0026lt;hostname\u0026gt;/trace/DB*\u0026lt;SID\u0026gt;/backint.log |\nHouse-keeping activities for SAP HANA backups: SAP note summarizes steps to maintain backup catalog house-keeping activities, and resolution for large backup catalog size to avoid SAP HANA Out Of Memory (OOM) errors. To learn more about SAP BASIS housekeeping activities for SAP HANA backups, please read the SAP Note: 3411878.\nStay updated with AWS Backint Agent version: Amazon Simple Notification Service (Amazon SNS) can notify you when new versions of AWS Backint agent or AWS Backint installer are released. To learn more, please visit Install and configure AWS Backint Agent for SAP HANA – Update to the newest version or install a previous version of AWS Backint agent.\nStay updated with SAP HANA database and Operating system version: SAP note summarizes the list of SAP HANA database versions and supported operating systems. To learn more, please read the SAP note: 2235581.\nConclusion This blog outlined strategies and best practices for implementing AWS Backup in SAP HANA HA deployments. It covered fully-managed capabilities such as scheduled backups, simplified recovery for HA deployments, cross-region cross-account copies for DR and ransomware protection, compliance adherence with Audit Manager, and CloudWatch metrics for monitoring jobs and recovery points. Cost optimization through lifecycle policies and retention period reviews was also addressed. The importance of regular strategy reviews, audits, and refinements based on evolving business needs was emphasized. These practices establish a robust, efficient, and compliant AWS Backup strategy for SAP HANA HA deployments, enhancing data protection and operational resilience.\nTo get started with the AWS Backup service, we recommend that you review the documentation, blog and workshop below.\nSAP HANA databases on Amazon EC2 instances backup Automate and Simplify SAP HANA Backups with AWS Backup Simplify SAP Backups with AWS Services Perform Cross-Region, Cross-Account Backup and Restore of SAP HANA database using AWS Backup for SAP AWS Backup for SAP HANA High-Availability To learn why thousands of customers trust AWS to migrate, modernize, and innovate with their SAP workloads, visit the SAP on AWS page.\nCredits I would like to thank the following team members for their contributions: Nerys Olver, Dhrumin Shah, Adam Hill, Spencer Martenson, David Rocha, Adam Kaminski and Balaji Krishna.\nTAGS: #saponaws, #awsforsap, AWS Backup, Backup, SAP, SAP on AWS, AWS Backint, SAP HANA, SAP High Availability, SAP on AWS\n"},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Extend the Amazon Q Developer CLI with Model Context Protocol (MCP) for Richer Context by Brian Beach | on 29 APR 2025 | in Amazon Aurora, Amazon Q Developer, Announcements, PostgreSQL compatible, RDS for PostgreSQL\nEarlier today, Amazon Q Developer announced Model Context Protocol (MCP) support in the command line interface (CLI). Developers can connect external data sources to Amazon Q Developer CLI with MCP support for more context-aware responses. By integrating MCP tools and prompts into Q Developer CLI, you get access to an expansive list of pre-built integrations or any MCP Servers that support stdio. This extra context helps Q Developer write more accurate code, understand your data structures, generate appropriate unit tests, create database documentation, and execute precise queries, all without needing to develop custom integration code. By extending Q Developer with MCP tools and prompts, developers can execute development tasks faster, streamlining the developer experience. At AWS, we’re committed to supporting popular open source protocols for agents like Model Context Protocol (MCP) proposed by Anthropic. We’ll continue to support this effort by extending this functionality within the Amazon Q Developer IDE plugins in the coming weeks.\nIntroduction I’m always on the lookout for tools and technologies that can streamline my workflow and unlock new capabilities. That’s why I was excited about the recent addition of Model Context Protocol (MCP) support in the Amazon Q Developer command line interface (CLI). MCP is an open protocol that standardizes how applications can seamlessly integrate with LLMs, providing a common way to share context, access data sources, and enable powerful AI-driven functionality. You can read more about MCP in this introduction.\nQ Developer has had the ability to use tools for a while. I previously discussed the ability to run CLI commands and describe AWS resources. With the Q Developer CLI’s support for MCP tools and prompts, I now have the ability to add additional tools. For example, while I have had the ability to describe my AWS resources, I also need to describe database schemas, message formats, etc. to build an application. Let’s see how I can configure MCP to provide this additional context.\nIn this post, I will configure an MCP server to provide Q Developer with my database schema for a simple Learning Management System (LMS) that I am working on. While Q Developer is great at writing SQL, it does not know the schema of my database. The table structure and relationships are stored in the database and are not part of the source code of my project. Therefore, I am going to use an MCP server that can query the database schema. Specifically, I am using the official PostgreSQL reference implementation to connect to my Amazon Relational Database Service (RDS). Let’s get started.\nBefore Model Context Protocol Prior to the introduction of MCP support, the Q Developer CLI provided a set of native tools, including the ability to execute bash commands, interact with files and the file system, and even make calls to AWS services. However, when it came to querying a database, the CLI was limited in its capabilities.\nFor example, prior to configuring the MCP server, I asked Q Developer to “Write a query that lists the students and the number of credits each student is taking.” In the following image you can see that Q Developer could only provide a generic SQL query, as it lacked the specific knowledge of the database schema for my LMS.\nWhile this is a great start, I know that Q developer could do so much more if it knew the database schema.\nConfiguring Model Context Protocol The introduction of MCP support in the Q Developer CLI allows me to easily configure MCP servers. I configure one or more MCP servers in a file called mcp.json. I can store the configuration in my home directory (e.g. ~/.aws/amazonq/mcp.json) and it is applied to all projects on my machine. Alternatively, I can store the configuration in the workspace root (e.g. .amazonq/mcp.json) so it is shared among project members. Here is an example of the configuration for the PostgreSQL MCP server.\nJSON { \u0026#34;mcpServers\u0026#34;: { \u0026#34;postgres\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;npx\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-y\u0026#34;, \u0026#34;@modelcontextprotocol/server-postgres\u0026#34;, \u0026#34;postgresql://USERNAME:PASSWORD@HOST:5432/DBNAME\u0026#34; ] } } } With the MCP server configured, let’s see how Amazon Q Developer enhances my experience.\nAfter Model Context Protocol First, I start a new Q Developer session and immediately see the benefits. In addition to the existing tools, Q Developer now has access to PostgreSQL as shown in the following image. This means I can easily explore the schema of my database, understand the structure of the tables, and even execute complex SQL queries, all without having to write any additional integration code.\nLet’s test the MCP server by asking Q Developer to “List the database tables.” As you can see in the following example, Q Developer now understands that I am asking about the PostgreSQL database, and uses the MCP server to list my three tables: students, courses, and enrollment.\nLet’s go back to the example from earlier in this post. Now, when I ask Q Developer to “Write a query that lists the students and the number of credits each student is taking,” it no longer responds with a generic query. Instead, Q Developer first describes the relevant tables in my database, generates the appropriate SQL query, and then executes it, providing me with the desired results.\nOf course, Q Developer can do a lot more than just write queries. Q Developer can use the MCP server to write Java code that accesses the database, create unit tests for the data layer, document the database, and much more. For example, I asked Q Developer to “Create an entity-relationship (ER) diagram using Mermaid syntax.” Q Developer was able to generate a visual representation of the database schema, helping me better understand the relationships between the various entities.\nThe integration of MCP into the Q Developer CLI has significantly streamlined my workflow by allowing me to add additional tools as needed.\nConclusion The addition of MCP support in the Amazon Q Developer CLI provides a standardized way to share context and access data sources. In this post, I’ve demonstrated how I can use the Q Developer CLI’s MCP integration to quickly set up a connection to a PostgreSQL database, explore the schema, and generate complex SQL queries without having to write any additional integration code. Moving forward, I’m excited to see how you can leverage MCP to further enhance your development workflow. I encourage you to explore the MCP capabilities and the AWS MCP Servers repository on GitHub.\nTAGS: Developer Tools, Development\n"},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #1 : AI/ML/GenAI on AWS Workshop” Event Objectives The workshop aimed to translate the often-complex domains of Machine Learning and Generative AI into practical, applicable knowledge for developers, engineers, and technology practitioners. Participants received an end-to-end look at the AWS ecosystem, demonstrating how Amazon SageMaker supports every phase of the machine learning workflow, and how Amazon Bedrock streamlines access to powerful Generative AI models. Beyond theory, the event emphasized real practice through live demonstrations, offering direct exposure to SageMaker Studio and the process of building a functional Generative AI chatbot with Bedrock. The sessions highlighted how modern AI technologies can be transformed into meaningful business outcomes, illustrating their relevance to the rapidly evolving tech landscape in Vietnam. Speakers Dinh Le Hoang Anh Lam Tuan Kiet Danh Hoang Hieu Nghi Key Highlights AWS AI/ML Services Overview Content: Speakers covered essential components such as data preparation, annotation workflows, model training, hyperparameter optimization, deployment patterns, and integrated MLOps tooling that supports scalable and maintainable machine learning operations. Key Moment: A central highlight was the live SageMaker Studio demo. Seeing the unified interface in action helped clarify how each ML stage connects and is managed through a single platform. Generative AI in Practice with Amazon Bedrock Content: The discussion provided a structured exploration of: Foundation Models: Practical comparison of Claude, Llama, Titan, and guidelines on choosing the right model for specific requirements. Prompt Engineering: Methods to design high-quality prompts, including strategies such as Chain-of-Thought and guided examples. Retrieval-Augmented Generation (RAG): How to link GenAI models to enterprise knowledge sources for contextual, accurate outputs. Bedrock Agents \u0026amp; Safety Controls: Building multi-step autonomous workflows and applying Guardrails for responsible AI usage. Key Moment: The live demo of constructing a GenAI chatbot using Bedrock served as the capstone experience, showing how quickly a prototype can be built. Key Takeaways Traditional Machine Learning Holistic Workflow Understanding: You gained clarity on how to manage an ML project end-to-end using Amazon SageMaker, from raw dataset ingestion to production deployment. Operational Excellence with MLOps: You saw how automation and governance tools within SageMaker help streamline model development, improve reproducibility, and support large-scale operations. Generative AI Model Selection \u0026amp; Prompt Crafting: You learned practical considerations for choosing foundation models and applying effective prompting techniques. Integrating Internal Knowledge: You now understand how RAG architectures enable AI to leverage proprietary data, resulting in more relevant and precise outputs. Building Safe, Automated AI Solutions: You gained insight into Bedrock Agents for multi-step workflows and Guardrails for ensuring AI systems operate within organizational safety guidelines. Event Experience Spending the morning at the AWS Vietnam office offered a refreshing blend of learning, inspiration, and technical exploration.\nEstablishing the Fundamentals The SageMaker segment provided a strong structural foundation. Instead of abstract theory, you were guided through an organized and repeatable ML development flow. The SageMaker Studio walkthrough was especially impactful, turning complex workflows into something tangible and intuitive.\nExploring the Future with GenAI The Bedrock session introduced a sense of excitement and forward momentum. Although topics like RAG and autonomous agents are often challenging, the presenters explained them in a relatable and practical way. The live chatbot demo delivered the defining moment of the event—witnessing an AI application come to life in real time underscored how accessible and empowering Generative AI has become.\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #2 : DevOps on AWS Workshop” Event Objectives The workshop aimed to help participants understand DevOps as a culture and philosophy—not just a toolkit—grounded in collaboration, speed, and continuous improvement, reinforced by key indicators such as DORA metrics. Attendees received a comprehensive walkthrough of the AWS DevOps toolchain, covering every stage of the software lifecycle from code management to production operations. Rather than focusing solely on theory, the event featured practical demos showing how to build CI/CD pipelines, deploy infrastructure as code, and run containerized applications. The sessions highlighted how to monitor, trace, and troubleshoot systems effectively in distributed, cloud-native environments. Speakers Truong Quang Tinh Van Hoang Kha Quoc Bao Phuc Thinh Dai Vi Long Huynh Quy Pham Nghiem Le Key Highlights From Code to Cloud: Automating the CI/CD Pipeline Content: Focused on building a complete CI/CD workflow using AWS DevOps services: CodeCommit for version control and recommended branching strategies. CodeBuild to automate build and testing stages. CodeDeploy with modern deployment techniques such as Blue/Green, Rolling, and Canary. CodePipeline to orchestrate the entire process into a unified automation flow. Key Moment: The live demonstration of a full pipeline—from code commit to production deployment—helped illustrate how each step connects to form a seamless delivery process. Laying the Foundation: Infrastructure as Code (IaC) Content: Shifted toward cloud resource management using code-driven approaches: AWS CloudFormation and its declarative templates. AWS CDK, enabling infrastructure creation through familiar programming languages. Key Moment: Seeing CloudFormation and CDK demonstrated side-by-side made it easier to understand their practical differences and helped participants determine when to use each tool. Deploying Modern Applications: The Container Ecosystem Content: Explored the role of containers in modern application architectures: Managing images with Amazon ECR. Comparing Amazon ECS (fully managed operations) and Amazon EKS (maximum flexibility). Introducing AWS App Runner for simplified deployments without managing infrastructure. Key Moment: The session’s microservices case study provided a clear look at the trade-offs among App Runner, ECS, and EKS in real-world scenarios. Closing the Loop: Full-Stack Observability Content: The final session focused on monitoring and tracing applications in production: Amazon CloudWatch for metrics and logs. AWS X-Ray for end-to-end distributed tracing. Best practices for alerts, dashboards, and on-call procedures. Key Moment: The live tracing demo—following a single request across multiple microservices—highlighted how observability uncovers bottlenecks that traditional monitoring often misses. Key Takeaways Automation \u0026amp; Pipelines Building a Complete CI/CD Workflow: Learned how to design and operate a fully automated delivery pipeline that accelerates deployments while reducing manual errors. Infrastructure as Code Fundamentals: Gained an understanding of how to manage cloud environments through version-controlled templates and code, along with clear guidance on when to choose CloudFormation or the AWS CDK. Modern Application Deployment Containerization on AWS: Received a practical framework for selecting the right container service—ECS, EKS, or App Runner—based on team expertise, architectural needs, and operational complexity. Advanced Deployment Techniques: Explored safe-release strategies like feature flags, A/B testing, and progressive rollouts to minimize risks during updates. System Reliability \u0026amp; Operations Building Observable Systems: Learned how metrics, logs, and traces work together to reveal system behavior and accelerate troubleshooting in distributed environments. Applying DevOps Best Practices: Gained insight into effective incident response, postmortems, and developing a culture centered on learning and improvement. Event Experience The “DevOps on AWS” workshop delivered a full-day immersion into the realities of building, shipping, and operating modern applications.\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Sources 2 Get to know FCJ members - Get to know team members, exchange and share information 8/9/2025 8/9/2025 3 Learn more about AWS: + Compute + Storage + Networking + \u0026hellip; 9/9/2025 9/9/2025 https://cloudjourney.awsstudygroup.com/ 4 Module 01 - 01 10/9/2025 10/9/2025 Module 01 - 01 5 Module 01 - 02 11/9/2025 11/9/2025 Module 01 - 02 6 Module 01 - 03 12/9/2025 12/9/2025 Module 01 - 03 Week 1 Achievements: Get along with the members of FCJ, mentors and the team.\nUnderstood the fundamentals of AWS and learned about its core service categories:\nCompute Storage Networking Database \u0026hellip; Completed some components of Module 01:\nExplored what cloud computing is and its core concept. Gained insight into the advantages of cloud computing, including reducing costs, speeding up development, flexible resource scaling, and expanding services worldwide. Built foundational knowledge of AWS and the upcoming cloud learning path. "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Finish Module 1 and Create AWS Account Tasks to be carried out this week: Day Task Start Date Completion Date Reference Sources 2 Module 01 - 04 15/9/2025 15/9/2025 Module 01 - 04 3 Module 01 - 05 16/9/2025 16/9/2025 Module 01 - 05 4 Module 01 - 06 17/9/2025 17/9/2025 Module 01 - 06 5 Module 01 - 07 18/9/2025 18/9/2025 Module 01 - 07 6 Complete Lab 01: Create AWS Account 19/9/2025 19/9/2025 Lab01 Week 2 Achievements: Finish Module 1\nGained knowledge of the AWS Global Infrastructure, including:\nData Centers Availability Zones (AZs) Regions (a region comprises a minimum of 3 AZs) Edge Locations CloudFront WAF (Web Application Firewall) Route 53 Explored AWS management tools:\nAWS Console - Root Login AWS Management Console - IAM Login Management Console - Service Search Management Console - Support Center AWS Command Line Interface (CLI) AWS SDKs (Software Development Kits) Learned about cost optimization methods when using AWS services:\nLeveraging discounted pricing models: Reserved Instances Savings Plans Spot Instances Practiced creating an AWS account. Conducted supplementary research on the AWS Well-Architected Framework. Successfully created and configured an AWS Free Tier account:\nSet up MFA (Multi-Factor Authentication) for the root account. Familiarized myself with creating an Admin Group and an Admin User. Configured authentication for the Admin User by:\nGenerating an Access Key. Managing the Secret Key (creation/deletion). "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Finish Module 2 Tasks to be carried out this week: Day Task Start Date Completion Date Reference Source Mon Module 02 - 01 22/9/2025 22/9/2025 Module 02 - 01 Tue Module 02 - 02 23/9/2025 23/9/2025 Module 02 - 02 Wed Module 02 - 03 24/9/2025 24/9/2025 Module 02 - 03 Thu Design appication with Figma 25/9/2025 25/9/2025 Fri Design appication with Figma 26/9/2025 26/9/2025 Week 3 Achievements: Explored, reviewed, and gained a solid grasp of VPC concepts\nDeveloped a clearer understanding of subnet types: public subnets and private subnets\nStudied various VPC-related components and services:\nRoute Tables Elastic Network Interfaces (ENIs) Elastic IP Addresses (EIPs) Internet Gateway NAT Gateway Examined security mechanisms within VPC:\nSecurity Groups Network Access Control Lists (NACLs) VPC Peering connections VPC Flow Logs Transit Gateway Learned about connectivity options between on-premises/data center infrastructures and AWS:\nSite-to-Site VPN AWS Direct Connect Studied the Elastic Load Balancing (ELB) service:\nSession stickiness (Sticky Sessions) Application Load Balancer (ALB) Network Load Balancer (NLB) Classic Load Balancer Gateway Load Balancer Gained knowledge of VPC Endpoints:\nUnderstood their core purpose and functionality Learned about the two main types of endpoints:\nInterface Endpoints Gateway Endpoints "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Complete Lab 03 Tasks to be carried out this week: Day Task Start Date Completion Date Reference Sources 2 Lab03-01 Lab03-01.1 Lab03-01.2 Lab03-01.3 Lab03-01.4 29/9/2025 29/9/2025 Lab03-01 Lab03-01.1 Lab03-01.2 Lab03-01.3 Lab03-01.4 3 Lab03-02.1 Lab03-02.2 Lab03-02.3 30/9/2025 30/9/2025 Lab03-02.1 Lab03-02.2 Lab03-02.3 4 Lab03-03.1 Lab03-03.2 Lab03-03.3 Lab03-03.4 Lab03-03.5 01/10/2025 01/10/2025 Lab03-03.1 Lab03-03.2 Lab03-03.3 Lab03-03.4 Lab03-03.5 5 Lab03-04.1 Lab03-04.2 Lab03-04.3 Lab03-04.5 02/10/2025 02/10/2025 Lab03-04.1 Lab03-04.2 Lab03-04.3 Lab03-04.5 6 Complete Lab 03 03/10/2025 03/10/2025 Lab03 Week 4 Achievements: Finished Lab 03 with the following accomplishments:\nGained hands-on experience setting up a VPC and configuring its subnets. Worked with Route Tables, updated Security Group settings, and explored how NACLs operate. Launched an EC2 instance and confirmed network connectivity. Used MobaXterm to establish the connection. Set up both a NAT Gateway and a VPC Endpoint. Also performed connectivity testing for the Endpoint. Note: To access the private instance, the inbound rules of its Security Group needed to be adjusted to allow SSH traffic coming from the Endpoint\u0026rsquo;s Security Group. "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Complete Lab 10, 19 Translate AWS Blogs Tasks to be carried out this week: Day Task Start Date Completion Date Reference Sources 2 - Complete Lab 10 06/10/2025 06/10/2025 Lab10 3 - Complete Lab 19 07/10/2025 07/10/2025 Lab19 4 - Translate Blog 1 08/10/2025 08/10/2025 Blog 1 5 - Translate Blog 2 09/10/2025 09/10/2025 Blog 2 6 - Translate Blog 3 10/10/2025 10/10/2025 Blog 3 Week 5 Achievements: Completed Lab 10 with the following outcomes:\nReceived an overview of the AWS CloudFormation service. Gained hands-on practice by building a template, updating a Security Group (SG), and accessing an EC2 instance through a Remote Desktop Gateway Server (RDGW). Worked on setting up a Directory Service. Explored Route 53 functionality:\nConfigured a Route 53 Outbound Endpoint. Created DNS Resolver Rules. Set up an Inbound Endpoint for Route 53. Verified that all connections were working correctly afterward. Successfully established communication between an on-premises environment and the AWS cloud using Directory Service and a Remote Desktop Gateway Server. Completed Lab 19 with the following achievements:\nUnderstood the concept of VPC isolation and the purpose of establishing VPC Peering for secure, private communication between VPCs. Successfully configured a VPC Peering connection between two VPCs to enable direct traffic routing via private IP. Practiced working with Network ACLs as subnet-level stateless firewalls and updated ACL rules to allow required traffic. Implemented routing updates in Route Tables to enable bidirectional communication between the two VPCs. Configured Cross-Peer DNS to allow resources in each VPC to resolve DNS names across the peering connection. Completed the full workflow: preparation → updating Network ACLs → creating the Peering connection → configuring route tables → enabling Cross-Peer DNS. "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Design Project UI Tasks to be carried out this week: Day Task Start Date Completion Date Resources 2 - Design Project UI with Figma 13/10/2025 13/10/2025 3 - Design Project UI with Figma 14/10/2025 14/10/2025 4 - Design Project UI with Figma 15/10/2025 15/10/2025 5 - Design Project UI with Figma 16/10/2025 16/10/2025 6 - Design Project UI with Figma 17/10/2025 17/10/2025 Week 6 Achievements: "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Design Project UI Tasks to be carried out this week: Day Task Start Date Completion Date Resources 2 - Design Project UI with Figma 20/10/2025 20/10/2025 3 - Design Project UI with Figma 21/10/2025 21/10/2025 4 - Design Project UI with Figma 22/10/2025 22/10/2025 5 - Design Project UI with Figma 23/10/2025 23/10/2025 6 - Design Project UI with Figma 24/10/2025 24/10/2025 Week 7 Achievements: "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Start working on the final project. Prepare for the midterm exam. Tasks to be carried out this week: Day Task Start Date Completion Date Resources 2 - Started working on Final Project 27/10/2025 27/10/2025 3 - Work on Final Project 28/10/2025 28/10/2025 4 - Review for the midterm exam 29/10/2025 29/10/2025 5 - Review for the midterm exam 30/10/2025 30/10/2025 6 - Midterm exam 31/10/2025 31/10/2025 Week 8 Achievements: Initiated brainstorming sessions for the Final Project. Conducted research on the services the team plans to use in the Final Project. Designed the system architecture incorporating the AWS services selected for the Project. "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Continue working on the final project. Tasks to be carried out this week: Day Task Start Date Completion Date Resources 2 - Work on Final Project 03/11/2025 03/11/2025 3 - Work on Final Project 04/11/2025 04/11/2025 4 - Work on Final Project 05/11/2025 05/11/2025 5 - Work on Final Project 06/11/2025 06/11/2025 6 - Work on Final Project 07/11/2025 07/11/2025 Week 9 Achievements: "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Finish Module 1 and Create AWS Account\nWeek 3: Finish Module 2 and Design Project UI\nWeek 4: Complete Lab 03\nWeek 5: Finish Module 04 and Translate AWS Blogs\nWeek 6: Design Project UI\nWeek 7: Design Project UI\nWeek 8: Review and take mid-term exam\nWeek 9: Working on final Project\nWeek 10: Working on final Project\nWeek 11: Working on final Project\nWeek 12: Working on and finish final Project\n"},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/5-workshop/5.2-prerequiste/","title":"2. Preparation Steps","tags":[],"description":"","content":"Objectives Create an S3 bucket Upload website resources to the S3 bucket Contents 5.2.1 - Create S3 bucket 5.2.2 - Upload resources "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/5-workshop/5.7-cloudfront/5.7.2-config-cloudfront/","title":"Configure CloudFront","tags":[],"description":"","content":"Use AWS Management Console to create a CloudFront distribution and configure this service to serve the S3 Bucket we created earlier.\nOpen CloudFront Console\nIn AWS Console, search for CloudFront From the dashboard, click on Create a CloudFront distribution. Configure CloudFront\nIn the plan selection interface, choose Free plan (note: this step might vary in newer console versions)\nSelect Next Distribution name: custom name (or workshop-demo)\nDistribution type: select Single website or app\nSelect Next On Specify Origin page\nOrigin type: select Amazon S3 Origin: select created S3 bucket Click Choose Then click Next Continue Next Review configuration, and then select Create distribution Then wait a few minutes for CloudFront to initialize\nGo to S3, into Permissions and scroll down to find Bucket policy Optional: If not working, try the following methods: Turn off S3 Static Website Hosting Then go to configure CloudFront Click on CloudFront ID In General, select Edit\nAdd index.html to Default root object\nThen select Save changes Then wait a few more minutes for CloudFront to re-initialize, and try opening the URL again in a new tab (or new browser) "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/2-proposal/","title":"Proposal","tags":[],"description":"","content":"SnapResume Platform 1. Project Summary SnapResume is an intelligent, AI-powered resume-building platform designed to help job seekers create professional, ATS (Applicant Tracking System) compliant resumes quickly and efficiently. The platform offers modern design templates, intuitive editing tools, and a smart recommendation system that optimizes content to match job descriptions.\nThe key differentiator is the integration of an AI Search \u0026amp; Recommendation Engine using AWS Bedrock (Claude 3 Sonnet). This allows the system to automatically analyze job descriptions and suggest the most relevant resume sections from the user\u0026rsquo;s experience data repository. The system is built entirely on a Serverless architecture, ensuring infinite scalability and optimized costs.\n2. Problem Statement What is the problem? Formatting Difficulties: Job seekers often spend hours formatting text in Word or complex design tools without achieving a professional look. Difficulty in Selecting Information: Candidates often have extensive experience but struggle to select what is most relevant to a specific position. Sending a \u0026ldquo;generic\u0026rdquo; CV to every company reduces the chance of acceptance. ATS Rejection: Manually designed resumes often contain graphic elements or table structures that make them unreadable to employers\u0026rsquo; automated Applicant Tracking Systems (ATS). The Solution SnapResume addresses these issues with a modern React web application built on the AWS Serverless platform.\nVisual Editor: Simple drag-and-drop or form-filling interface with Real-time Preview. AI Smart Matcher: Uses AWS Bedrock to analyze the relevance between the candidate\u0026rsquo;s experience and job requirements, thereby recommending a list of skills and projects to include in the CV. ATS Compliant: Templates are designed to be machine-friendly while remaining aesthetically pleasing for human readers. Centralized Management: Stores a library of sections and allows for \u0026ldquo;assembling\u0026rdquo; multiple resume versions based on specific needs. 3. Solution Architecture The system utilizes Serverless and Event-driven architecture on AWS. Architecture Overview Frontend: React + Vite + Ant Design (hosted on Amplify and accelerated via CloudFront). API Layer: Amazon API Gateway + AWS Lambda (Node.js/TypeScript). Database: Amazon DynamoDB for storing Users, Resumes, Sections, and Templates. Authentication: Amazon Cognito (User Pools \u0026amp; Identity Pools). AI Engine: Amazon Bedrock (Claude 3 Sonnet) for analysis and recommendation tasks. Storage: Amazon S3 for storing profile pictures and PDF files. 4. Technical Implementation Technology Stack Frontend: ReactJS, Ant Design, CSS Modules, html2pdf.js for PDF export. Backend: NodeJS, TypeScript, Express (running inside Lambda). IaC: Terraform for managing the entire infrastructure. Phases Phase 1: Core Foundation (Completed) Setup AWS Infrastructure (Terraform). Build Authentication (Cognito). Basic CRUD for Resumes \u0026amp; Sections (DynamoDB + Lambda). Phase 2: Editor \u0026amp; Templates (Completed) Develop Universal Editor Interface (Extension/Web). Build Dynamic Template System. PDF Export Feature (html2pdf.js). Phase 3: AI Integration \u0026amp; Polish (In Progress) Integrate Amazon Bedrock for \u0026ldquo;AI Recommendation\u0026rdquo; feature. Optimize UX/UI (Features Page, Editor Flow). Extension Integration (Web Clipper flow). 5. Budget Estimate (AWS Costs) The Serverless pricing model ensures costs are proportional to usage. AI costs are focused on analyzing large input data (Input Tokens).\nAI Cost Assumptions The estimated costs for the Amazon Bedrock service (utilizing the Claude 3 Sonnet model) are calculated based on the following real-world usage scenario (MVP):\n1. Usage Frequency \u0026amp; Volume Processing Rate: The system processes an average of 1 request per minute (1 request/minute). Operating Time: The system is expected to be under high load or actively used for 2 hours/day (120 minutes/day). Total Monthly Volume: 120 requests/day × 30 days = 3,600 requests/month. 2. Data Size (Token Size) per Request Input (Input Tokens): Average of 500 tokens/request. Includes: Data extracted from the user\u0026rsquo;s current CV and the content of the Job Description to be analyzed. Output (Output Tokens): Average of 600 tokens/request. Includes: Analysis results, editing suggestions, and data returned in structured JSON format. 3. Unit Pricing (Pricing - Claude 3 Sonnet) Input: $0.003 / 1,000 tokens (equivalent to $3.00 / 1 million tokens). Output: $0.015 / 1,000 tokens (equivalent to $15.00 / 1 million tokens). AI Cost Breakdown Total Monthly Volume = Frequency _ Duration _ 30 days Input Token Cost = Total Monthly Volume _ Input Tokens per Request _ ($3 / 1 million tokens) Output Token Cost = Total Monthly Volume _ Output Tokens per Request _ ($15 / 1 million tokens) Total Cost = Input Token Cost + Output Token Cost Low traffic Processing Rate: 1 request/minute Operating Time: 2 hours/day High traffic Processing Rate: 2 request/minute Operating Time: 3 hours/day Service Pricing Model Low Traffic (MVP) Medium Traffic Amazon S3 Storage $0.28 $0.77 CloudFront CDN Free Tier Free Tier API GW + Lambda Compute Free Tier $2.80 DynamoDB Database Free Tier $6.25 Cognito Auth Free Tier Free Tier Amazon Bedrock (AI) Tokens $37.8 $113.4 WAF + Route53 Security $12.60 $12.60 Total Cost / Month ~ $50.68 ~ $135.82 6. Risk Assessment Security \u0026amp; Privacy Risks (High) Issue: The Recommendation feature requires sending the user\u0026rsquo;s entire Section data to Bedrock for analysis. Mitigation: AWS Bedrock complies with regulations ensuring customer data is not used to train base models. Data encryption at rest in DynamoDB. Cost Risks (Medium) Issue: Users spamming the \u0026ldquo;Recommend\u0026rdquo; button with long Job Descriptions causing high Input Token usage. Mitigation: Limit the length of Job Description Input. Implement Quota limits (e.g., 10 analysis calls/day for Free accounts). Architectural Risks (Low) Issue: Lambda cold starts slowing down the initial user experience. Mitigation: Use Lambda SnapStart (for Java) or Provisioned Concurrency if necessary (though Node.js cold starts are generally quite fast). 7. Expected Outcomes Ownership of a complete SaaS platform for the recruitment market. Solving the \u0026ldquo;Matching\u0026rdquo; problem between candidates and jobs, rather than being just a pure text editing tool. Leveraging AWS AI power to create real value (Smart Search) instead of generic \u0026ldquo;writing assistance\u0026rdquo; features. Attached Documents\n"},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/5-workshop/5.2-prerequiste/5.2.2-download-source-code/","title":"Upload Resources","tags":[],"description":"","content":"Upload Data Download the source code we will use in this workshop.\nDownload link: Source code\nIn the interface of the newly created S3 bucket. Currently, we see there are no objects. Select Upload to upload data (the source code downloaded and extracted in the previous step) In the Upload interface Open the window containing the folders and files downloaded and extracted in the step above. Press Ctrl A to select all folders and files inside the workshop-demo-main directory. Drag all selected folders and files and drop them into the upload section of the S3 Bucket. Wait for about 5 minutes for the data upload to the S3 bucket to complete.\nInterface after upload is finished: View directory in S3 bucket "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Continue working on the final project. Tasks to be carried out this week: Day Task Start Date Completion Date Resources 2 - Work on Final Project 10/11/2025 10/11/2025 3 - Work on Final Project 11/11/2025 11/11/2025 4 - Work on Final Project 12/11/2025 12/11/2025 5 - Work on Final Project 13/11/2025 13/11/2025 6 - Work on Final Project 14/11/2025 14/11/2025 Week 10 Achievements: "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Attend the AWS Cloud Mastery #2 event. Complete the Project Proposal and the final report. Tasks to be carried out this week: Day Task Start Date Completion Date Resources 2 - Attend the AWS Cloud Mastery #2 event: DevOps on AWS 17/11/2025 17/11/2025 3 - Work on Final Proposal 18/11/2025 18/11/2025 4 - Attend AWS event about Edge Netwok Services 19/11/2025 19/11/2025 5 - Work on Final Proposal 20/11/2025 20/11/2025 6 - Work on Final Proposal 21/11/2025 21/11/2025 Week 11 Achievements: "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Complete the Project Proposal and the final report. Tasks to be carried out this week: Day Task Start Date Completion Date Resources 2 - Work on Final Project 24/11/2025 24/11/2025 3 - Work on Final Project 25/11/2025 25/11/2025 4 - Work on Final Project 26/11/2025 26/11/2025 5 - Work on Final Project 27/11/2025 27/11/2025 6 - Work on Final Project 28/11/2025 28/11/2025 Week 12 Achievements: "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/5-workshop/5.3-s3-static-web/","title":"3. Enable S3 Static Website Hosting","tags":[],"description":"","content":"In this section, you will enable static website hosting feature from the S3 bucket.\nStep-by-Step Configuration Access Bucket Properties In your S3 bucket interface, select the Properties tab Find Static Website Hosting In the Properties interface:\nScroll down to find the Static website hosting section Select Edit to modify settings Configure Website Hosting Settings\nStatic website hosting: Select Enable Hosting type: Select Host a static website Index document: Enter index.html Error document (optional): Enter error.html if you have a custom error page Save Configuration Note: After enabling static website hosting, your bucket will have a website endpoint URL, but the content will not be accessible until you:\nUpload your website files Configure public access Set up appropriate bucket policies Verify Configuration What has been achieved Enabled static website hosting from S3 bucket Set up website hosting configuration Obtained website endpoint URL "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/5-workshop/5.7-cloudfront/5.7.3-deploy-cloudfront/","title":"Deploy CloudFront","tags":[],"description":"","content":" Check CloudFront\nIn CloudFront interface, select ID. At Distribution domain name section:\nEnsure CloudFront has finished deploying by checking content in Last modified section. Select square icon to copy URL. Open URL in new browser tab -\u0026gt; Successfully deployed CloudFront\n"},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Enhance governance with metadata enforcement rules in Amazon SageMaker This blog shows how metadata enforcement rules in Amazon SageMaker improve data governance by standardizing required information across publishing and access workflows. Much like building a structured data lake with modern architectural practices, these rules bring consistency, compliance, and clearer ownership to the data lifecycle. By enforcing key metadata and supporting custom approval flows, organizations can maintain higher data quality and streamline access, creating a more reliable and scalable foundation for analytics and AI.\nhttps://aws.amazon.com/vi/blogs/big-data/enhance-governance-with-metadata-enforcement-rules-in-amazon-sagemaker/\nBlog 2 - Enhance resiliency and data protection for SAP HANA high-availability deployments with Amazon Backup for SAP HANA This blog introduces how to build a resilient backup architecture for SAP HANA High-Availability deployments on AWS using AWS Backup. You will learn why backups are still essential beyond HA—for protection against corruption, errors, ransomware, and compliance needs—and how AWS Backup provides automated, consistent, and scalable data protection. The article also walks through setting up the environment, running backup/restore operations in a Pacemaker cluster, and applying best practices for performance, monitoring, disaster recovery, and governance to ensure secure and reliable SAP HANA operations on AWS.\nhttps://aws.amazon.com/blogs/awsforsap/enhance-resiliency-and-data-protection-for-sap-hana-high-availability-deployments-with-amazon-backup-for-sap-hana/\nBlog 3 - Extend the Amazon Q Developer CLI with Model Context Protocol (MCP) for Richer Context This blog introduces how MCP (Model Context Protocol) support in Amazon Q Developer CLI enhances developer productivity by enabling richer, context-aware workflows. With MCP tools and prompts, Q Developer can connect to external data sources—such as PostgreSQL—without custom integration code, allowing it to understand schemas, write accurate queries, generate tests, and document databases. By standardizing how Q Developer accesses and shares context, MCP expands its capabilities and streamlines development tasks, creating a faster, more integrated developer experience.\nhttps://aws.amazon.com/blogs/devops/extend-the-amazon-q-developer-cli-with-mcp/\n"},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/5-workshop/5.4-config-block-public/","title":"4. Configure Block Public Access","tags":[],"description":"","content":"Configure Block Public Access To access the hosted website, we need to reconfigure Block Public Access as follows:\nIn the S3 bucket interface: Select Permissions At this point, you will see Block all public access is in On state. In the Block public access interface: Uncheck Block all public access Select Save changes You have now completed turning off Block public access.\n"},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Mastery Series #1 : AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: 8:00, 15 November 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #2 : DevOps on AWS Workshop\nDate \u0026amp; Time: 8:00, 17 November 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/5-workshop/5.5-config-public-object/","title":"5. Configure Public Object","tags":[],"description":"","content":"Step-by-Step Configuration Access Bucket Permissions In your S3 bucket interface, select the Permissions tab Find Access Control List Settings Scroll down to find the Access control list (ACL) section\nYou will see Bucket owner enforced is currently selected This means ACLs are disabled and the bucket owner controls all objects Enable ACL for Object Level Control Select Edit in the Object Ownership section, then configure: Object ownership: Select ACLS enabled Acknowledgment: Check I acknowledge that ACLs will be restored Object ownership setting: Select Bucket owner preferred Select Save changes Verify ACL Configuration After saving, you will see ACLs enabled in the Object Ownership section.\nUpdated Configuration: Your bucket now supports ACLs, allowing you to set object-level permissions including public access. Make Objects Public Using ACL Navigate back to the Objects tab of the bucket:\nSelect the objects or folders you want to make public Select Actions from the toolbar Select Make public using ACL Confirm Public Access On the Make public confirmation page:\nReview the objects that will be made public Understand that these objects will be accessible by anyone Select Make public to confirm Final Warning: When you click “Make public”, these objects will immediately be accessible by anyone on the internet who knows the URL.\nVerify Public Configuration Success! Your objects are now publicly accessible.\n"},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/5-workshop/","title":"Workshop 5: S3 Static Website &amp; CloudFront","tags":[],"description":"","content":"S3 Static Website \u0026amp; CloudFront Hosting System In this workshop, we will build a static website hosting and delivery system using Amazon S3 and Amazon CloudFront.\nSystem Architecture The system consists of the following key components:\nAmazon S3 Bucket: Stores website source code (HTML, CSS, JS, Images). Configured with Static Website Hosting feature. Amazon CloudFront: Content Delivery Network (CDN) to accelerate website access for global users and reduce load on S3. Security: Access management via Block Public Access and Bucket Policies/ACLs. Workshop Contents The practical labs are divided into sections for easy following:\nOverview (5.1): Introduction to the workshop and services used. Preparation (5.2): Create S3 Bucket and upload sample travel website source code. S3 Hosting Config (5.3): Enable Static Website Hosting on the bucket. Security Config (5.4 - 5.5): Configure Block Public Access and ACL to make the website public. Verify Website (5.6): Access the website via S3 Endpoint. CloudFront Integration (5.7): Deploy CloudFront CDN to distribute website content. Cleanup (5.8): Delete resources to avoid incurring costs. "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/5-workshop/5.6-check-website/","title":"6. Verify Website","tags":[],"description":"","content":"Verify Website After successfully installing and configuring, the next step is to check the website.\nAccess the created S3 bucket: Select Object. Select the file index.html Find details of index.html: Select Object URL. Open URL in a new browser tab: Experience the travel website interface. "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from 08/09/2025 to 12/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in SnapResume project, through which I improved my skills in programming, analysis, reporting, communication, etc..\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ☐ ✅ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ☐ ✅ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Increase proactiveness in receiving and handling tasks. Improve problem-solving skills, analysis, and proposing better solutions. Enhance communication skills for clearer expression and more effective collaboration. Strengthen teamwork skills through discussion and supporting teammates. Expand professional knowledge. "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/5-workshop/5.7-cloudfront/","title":"7. CloudFront","tags":[],"description":"","content":"CloudFront is a CDN (Content Delivery Network) service from AWS. It helps reduce load for websites and applications by distributing content to locations closest to users.\nCloudFront has recently updated its new interface, as well as new features and services.\nIn this workshop, we will learn how to configure CloudFront and use it to distribute content to locations closest to users.\nContents 5.7.1 - Block Public Access 5.7.2 - Configure CloudFront 5.7.3 - Deploy CloudFront "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed.\nAdditional Questions What did you find most satisfying during your internship? Flexible schedule, the enthusiasm of the mentors. What do you think the company should improve for future interns?\nIncrease the amount of study seeions, events, ect.\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nYes, because currently AWS has global coverage, having the opportunity to intern here would be fortunate, as the job opportunities are higher.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? No Would you like to continue this program in the future? Yes Any other comments (free sharing): "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/5-workshop/5.8-cleanup/","title":"8. Cleanup Resources","tags":[],"description":"","content":"Cleanup Resources Empty S3 Bucket Access AWS S3. In Bucket name list, select the bucket related to the lab. Select Empty. In Empty bucket page, confirm and select Empty. Delete S3 Bucket Access AWS S3. Select S3 bucket related to the lab. Select Delete bucket. Delete CloudFront Access Amazon CloudFront. Check the box before Distributions ID, on top right - select Disable. This process will take a few minutes. After Disable is complete, select Delete. "},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://phuchoang2503.github.io/AWS_Intern_Workshop/tags/","title":"Tags","tags":[],"description":"","content":""}]